{"model": "meta-llama/Meta-Llama-3-8B", "generate_kwargs": {"num_return_sequences": 1, "max_new_tokens": 24}, "model_config": {"vocab_size": 128256, "max_position_embeddings": 8192, "hidden_size": 4096, "intermediate_size": 14336, "num_hidden_layers": 32, "num_attention_heads": 32, "num_key_value_heads": 8, "hidden_act": "silu", "initializer_range": 0.02, "rms_norm_eps": 1e-05, "pretraining_tp": 1, "use_cache": true, "rope_theta": 500000.0, "rope_scaling": null, "attention_bias": false, "attention_dropout": 0.0, "mlp_bias": false, "return_dict": true, "output_hidden_states": false, "output_attentions": false, "torchscript": false, "torch_dtype": "float16", "use_bfloat16": false, "tf_legacy_loss": false, "pruned_heads": {}, "tie_word_embeddings": false, "chunk_size_feed_forward": 0, "is_encoder_decoder": false, "is_decoder": false, "cross_attention_hidden_size": null, "add_cross_attention": false, "tie_encoder_decoder": false, "max_length": 20, "min_length": 0, "do_sample": false, "early_stopping": false, "num_beams": 1, "num_beam_groups": 1, "diversity_penalty": 0.0, "temperature": 1.0, "top_k": 50, "top_p": 1.0, "typical_p": 1.0, "repetition_penalty": 1.0, "length_penalty": 1.0, "no_repeat_ngram_size": 0, "encoder_no_repeat_ngram_size": 0, "bad_words_ids": null, "num_return_sequences": 1, "output_scores": false, "return_dict_in_generate": false, "forced_bos_token_id": null, "forced_eos_token_id": null, "remove_invalid_values": false, "exponential_decay_length_penalty": null, "suppress_tokens": null, "begin_suppress_tokens": null, "architectures": ["LlamaForCausalLM"], "finetuning_task": null, "id2label": {"0": "LABEL_0", "1": "LABEL_1"}, "label2id": {"LABEL_0": 0, "LABEL_1": 1}, "tokenizer_class": null, "prefix": null, "bos_token_id": 128000, "pad_token_id": null, "eos_token_id": 128001, "sep_token_id": null, "decoder_start_token_id": null, "task_specific_params": null, "problem_type": null, "_name_or_path": "meta-llama/Meta-Llama-3-8B", "transformers_version": "4.42.3", "model_type": "llama"}, "subjects_results": {"abstract_algebra": {"num_questions_subject": 100, "accuracy": 0.24, "num_readable_responses": 50, "input_tokens_avg": 702.84}, "anatomy": {"num_questions_subject": 135, "accuracy": 0.5779816513761468, "num_readable_responses": 109, "input_tokens_avg": 744.9633027522935}, "astronomy": {"num_questions_subject": 152, "accuracy": 0.7195121951219512, "num_readable_responses": 82, "input_tokens_avg": 885.7439024390244}, "business_ethics": {"num_questions_subject": 100, "accuracy": 0.6666666666666666, "num_readable_responses": 33, "input_tokens_avg": 782.6060606060606}, "clinical_knowledge": {"num_questions_subject": 265, "accuracy": 0.7534246575342466, "num_readable_responses": 146, "input_tokens_avg": 647.6095890410959}, "college_biology": {"num_questions_subject": 144, "accuracy": 0.7073170731707317, "num_readable_responses": 123, "input_tokens_avg": 806.4471544715448}, "college_chemistry": {"num_questions_subject": 100, "accuracy": 0.5161290322580645, "num_readable_responses": 31, "input_tokens_avg": 934.4193548387096}, "college_computer_science": {"num_questions_subject": 100, "accuracy": 0.41379310344827586, "num_readable_responses": 58, "input_tokens_avg": 1041.4827586206898}, "college_mathematics": {"num_questions_subject": 100, "accuracy": 0.3783783783783784, "num_readable_responses": 37, "input_tokens_avg": 979.3783783783783}, "college_medicine": {"num_questions_subject": 173, "accuracy": 0.6022727272727273, "num_readable_responses": 88, "input_tokens_avg": 899.7159090909091}, "college_physics": {"num_questions_subject": 102, "accuracy": 0.7567567567567568, "num_readable_responses": 37, "input_tokens_avg": 877.027027027027}, "computer_security": {"num_questions_subject": 100, "accuracy": 0.7162162162162162, "num_readable_responses": 74, "input_tokens_avg": 1044.1891891891892}, "conceptual_physics": {"num_questions_subject": 235, "accuracy": 0.5353535353535354, "num_readable_responses": 99, "input_tokens_avg": 580.7676767676768}, "econometrics": {"num_questions_subject": 114, "accuracy": 0.4657534246575342, "num_readable_responses": 73, "input_tokens_avg": 966.027397260274}, "electrical_engineering": {"num_questions_subject": 145, "accuracy": 0.5675675675675675, "num_readable_responses": 74, "input_tokens_avg": 643.1351351351351}, "elementary_mathematics": {"num_questions_subject": 378, "accuracy": 0.4264705882352941, "num_readable_responses": 68, "input_tokens_avg": 783.8823529411765}, "formal_logic": {"num_questions_subject": 126, "accuracy": 0.4462809917355372, "num_readable_responses": 121, "input_tokens_avg": 1185.3636363636363}, "global_facts": {"num_questions_subject": 100, "accuracy": 0.27586206896551724, "num_readable_responses": 29, "input_tokens_avg": 678.8620689655172}, "high_school_biology": {"num_questions_subject": 310, "accuracy": 0.7322834645669292, "num_readable_responses": 254, "input_tokens_avg": 955.7362204724409}, "high_school_chemistry": {"num_questions_subject": 203, "accuracy": 0.5584415584415584, "num_readable_responses": 77, "input_tokens_avg": 904.0}, "high_school_computer_science": {"num_questions_subject": 100, "accuracy": 0.65625, "num_readable_responses": 64, "input_tokens_avg": 945.3125}, "high_school_european_history": {"num_questions_subject": 165, "accuracy": 0.7115384615384616, "num_readable_responses": 104, "input_tokens_avg": 3408.2884615384614}, "high_school_geography": {"num_questions_subject": 198, "accuracy": 0.8051948051948052, "num_readable_responses": 154, "input_tokens_avg": 620.5194805194806}, "high_school_government_and_politics": {"num_questions_subject": 193, "accuracy": 0.8488372093023255, "num_readable_responses": 172, "input_tokens_avg": 850.6104651162791}, "high_school_macroeconomics": {"num_questions_subject": 390, "accuracy": 0.5807453416149069, "num_readable_responses": 322, "input_tokens_avg": 800.0341614906832}, "high_school_mathematics": {"num_questions_subject": 270, "accuracy": 0.4375, "num_readable_responses": 16, "input_tokens_avg": 805.75}, "high_school_microeconomics": {"num_questions_subject": 238, "accuracy": 0.6179775280898876, "num_readable_responses": 178, "input_tokens_avg": 733.5112359550562}, "high_school_physics": {"num_questions_subject": 151, "accuracy": 0.2727272727272727, "num_readable_responses": 44, "input_tokens_avg": 1060.409090909091}, "high_school_psychology": {"num_questions_subject": 545, "accuracy": 0.8173913043478261, "num_readable_responses": 460, "input_tokens_avg": 703.4760869565217}, "high_school_statistics": {"num_questions_subject": 216, "accuracy": 0.5081967213114754, "num_readable_responses": 122, "input_tokens_avg": 1216.827868852459}, "high_school_us_history": {"num_questions_subject": 204, "accuracy": 0.7905405405405406, "num_readable_responses": 148, "input_tokens_avg": 2777.4662162162163}, "high_school_world_history": {"num_questions_subject": 237, "accuracy": 0.791907514450867, "num_readable_responses": 173, "input_tokens_avg": 3170.057803468208}, "human_aging": {"num_questions_subject": 223, "accuracy": 0.655367231638418, "num_readable_responses": 177, "input_tokens_avg": 615.1694915254237}, "human_sexuality": {"num_questions_subject": 131, "accuracy": 0.7128712871287128, "num_readable_responses": 101, "input_tokens_avg": 644.019801980198}, "international_law": {"num_questions_subject": 121, "accuracy": 0.7608695652173914, "num_readable_responses": 92, "input_tokens_avg": 1046.3695652173913}, "jurisprudence": {"num_questions_subject": 108, "accuracy": 0.6547619047619048, "num_readable_responses": 84, "input_tokens_avg": 770.452380952381}, "logical_fallacies": {"num_questions_subject": 163, "accuracy": 0.7417218543046358, "num_readable_responses": 151, "input_tokens_avg": 795.2980132450331}, "machine_learning": {"num_questions_subject": 112, "accuracy": 0.39603960396039606, "num_readable_responses": 101, "input_tokens_avg": 798.2079207920792}, "management": {"num_questions_subject": 103, "accuracy": 0.8309859154929577, "num_readable_responses": 71, "input_tokens_avg": 508.5774647887324}, "marketing": {"num_questions_subject": 234, "accuracy": 0.8471615720524017, "num_readable_responses": 229, "input_tokens_avg": 643.6288209606987}, "medical_genetics": {"num_questions_subject": 100, "accuracy": 0.7346938775510204, "num_readable_responses": 49, "input_tokens_avg": 745.1020408163265}, "miscellaneous": {"num_questions_subject": 783, "accuracy": 0.8185483870967742, "num_readable_responses": 496, "input_tokens_avg": 522.0584677419355}, "moral_disputes": {"num_questions_subject": 346, "accuracy": 0.6320474777448071, "num_readable_responses": 337, "input_tokens_avg": 719.4213649851632}, "moral_scenarios": {"num_questions_subject": 895, "accuracy": 0.33884297520661155, "num_readable_responses": 847, "input_tokens_avg": 1109.2951593860685}, "nutrition": {"num_questions_subject": 306, "accuracy": 0.7258687258687259, "num_readable_responses": 259, "input_tokens_avg": 697.5714285714286}, "philosophy": {"num_questions_subject": 311, "accuracy": 0.7032258064516129, "num_readable_responses": 310, "input_tokens_avg": 826.3064516129032}, "prehistory": {"num_questions_subject": 324, "accuracy": 0.7622950819672131, "num_readable_responses": 244, "input_tokens_avg": 807.0901639344262}, "professional_accounting": {"num_questions_subject": 282, "accuracy": 0.38636363636363635, "num_readable_responses": 88, "input_tokens_avg": 1168.6022727272727}, "professional_law": {"num_questions_subject": 1534, "accuracy": 0.39372138652714195, "num_readable_responses": 1529, "input_tokens_avg": 2163.7782864617398}, "professional_medicine": {"num_questions_subject": 272, "accuracy": 0.6379310344827587, "num_readable_responses": 232, "input_tokens_avg": 1637.4568965517242}, "professional_psychology": {"num_questions_subject": 612, "accuracy": 0.6176991150442478, "num_readable_responses": 565, "input_tokens_avg": 996.9185840707964}, "public_relations": {"num_questions_subject": 110, "accuracy": 0.7241379310344828, "num_readable_responses": 87, "input_tokens_avg": 667.9080459770115}, "security_studies": {"num_questions_subject": 245, "accuracy": 0.7213930348258707, "num_readable_responses": 201, "input_tokens_avg": 1650.8507462686566}, "sociology": {"num_questions_subject": 201, "accuracy": 0.8540145985401459, "num_readable_responses": 137, "input_tokens_avg": 773.8686131386861}, "us_foreign_policy": {"num_questions_subject": 100, "accuracy": 0.8181818181818182, "num_readable_responses": 88, "input_tokens_avg": 829.5795454545455}, "virology": {"num_questions_subject": 166, "accuracy": 0.5367647058823529, "num_readable_responses": 136, "input_tokens_avg": 831.3235294117648}, "world_religions": {"num_questions_subject": 171, "accuracy": 0.80625, "num_readable_responses": 160, "input_tokens_avg": 541.05625}}}